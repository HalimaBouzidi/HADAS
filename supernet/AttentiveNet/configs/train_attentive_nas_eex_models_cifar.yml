#### models ####
arch: 'attentive_nas_eex_model'
supernet_checkpoint_path: "/home/hbouzidi/hbouzidi/hw_dynamic_nas/problems/AttentiveNet/attentive_nas_data/attentive_nas_cifar100.pth.tar"

exp_name: ""

pareto_models:
    supernet_checkpoint_path: "/home/hbouzidi/hbouzidi/hw_dynamic_nas/problems/AttentiveNet/attentive_nas_data/attentive_nas_cifar100.pth.tar"

    a0:
        model: "attentive_nas_a0"
        resolution: 192
        width: [16, 16, 24, 32, 64, 112, 192, 216, 1792]
        kernel_size: [3, 3, 3, 3, 3, 3, 3]
        expand_ratio: [1, 4, 4, 4, 4, 6, 6]
        depth: [1, 3, 3, 3, 3, 3, 1]
        block_ee: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
        num_ee: 12

    a1: 
        model: "attentive_nas_a1"
        resolution: 224
        width: [16, 16, 24, 32, 64, 112, 192, 216, 1984]
        kernel_size: [3, 3, 3, 5, 3, 5, 3]
        expand_ratio: [1, 4, 4, 4, 4, 6, 6]
        depth: [1, 3, 3, 3, 3, 3, 1]
        block_ee: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
        num_ee: 12

    a2:
        model: "attentive_nas_a2"
        resolution: 224
        width: [16, 16, 24, 32, 64, 112, 200, 224, 1984]
        kernel_size: [3, 3, 3, 3, 3, 5, 3]
        expand_ratio: [1, 4, 5, 4, 4, 6, 6]
        depth: [1, 3, 3, 3, 3, 4, 1]
        block_ee: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
        num_ee: 13

    a3:
        model: "attentive_nas_a3"
        resolution: 224
        width: [16, 16, 24, 32, 64, 112, 208, 224, 1984]
        kernel_size: [3, 3, 3, 5, 3, 3, 3]
        expand_ratio: [1, 4, 4, 4, 4, 6, 6]
        depth: [2, 3, 3, 4, 3, 5, 1]
        block_ee: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
        num_ee: 16

    a4:
        model: "attentive_nas_a4"
        resolution: 256
        width: [16, 16, 24, 32, 64, 112, 192, 216, 1984]
        kernel_size: [3, 3, 3, 5, 3, 5, 3]
        expand_ratio: [1, 4, 4, 5, 4, 6, 6]
        depth: [1, 3, 3, 4, 3, 5, 1]
        block_ee: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
        num_ee: 15

    a5:
        model: "attentive_nas_a5"
        resolution: 256
        width: [16, 16, 24, 32, 72, 112, 192, 216, 1792]
        kernel_size: [3, 3, 3, 5, 3, 3, 3]
        expand_ratio: [1, 4, 5, 4, 4, 6, 6]
        depth: [1, 3, 3, 3, 4, 6, 1]
        block_ee: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
        num_ee: 16

    a6:
        model: "attentive_nas_a6"
        resolution: 288
        width: [16, 16, 24, 32, 64, 112, 216, 224, 1984]
        kernel_size: [3, 3, 3, 3, 3, 5, 3]
        expand_ratio: [1, 4, 6, 5, 4, 6, 6]
        depth: [1, 3, 3, 4, 4, 6, 1]
        block_ee: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
        num_ee: 17

exit_threshold: 0.6
batch_size_per_gpu: 64

augment: "auto_augment_tf"

distributed: False

warmup_epochs: 5
epochs: 50
start_epoch: 0

#sync-batchnormalization, suggested to use in bignas
sync_bn: False

bn_momentum: 0
bn_eps: 1e-5

post_bn_calibration_batch_num: 64

models_save_dir: "/home/hbouzidi/hbouzidi/hw_dynamic_nas/problems/AttentiveNet/saved_models"

#### cloud training resources  ####
data_loader_workers_per_gpu: 4

weight_decay_weight: 0.00001
weight_decay_bn_bias: 0.

loss: "kl-div" # or "nll"

## =================== optimizer and scheduler======================== #
optimizer:
    method: sgd
    momentum: 0.9
    nesterov: True

lr_scheduler:
    method: "warmup_linear_lr"
    base_lr: 0.01
    clamp_lr_percent: 0.0

### Cifar-100 training dataset ###
dataset: 'cifar-100' 
dataset_dir: "/home/hbouzidi/hbouzidi/datasets/cifar-100"
n_classes: 100
drop_last: True

resume : ""
print_freq: 200
seed: 0

#attentive nas search space
# c: channels, d: layers, k: kernel size, t: expand ratio, s: stride, act: activation, se: se layer
supernet_config:
    use_v3_head: True
    resolutions: [192, 224, 256, 288]
    first_conv: 
        c: [16, 24]
        act_func: 'swish'
        s: 2
    mb1:
        c: [16, 24]
        d: [1, 2]
        k: [3, 5]
        t: [1]
        s: 1
        act_func: 'swish'
        se: False
    mb2:
        c: [24, 32]
        d: [3, 4, 5]
        k: [3, 5]
        t: [4, 5, 6]
        s: 2
        act_func: 'swish'
        se: False
    mb3:
        c: [32, 40] 
        d: [3, 4, 5, 6]
        k: [3, 5]
        t: [4, 5, 6]
        s: 2
        act_func: 'swish'
        se: True
    mb4:
        c: [64, 72] 
        d: [3, 4, 5, 6]
        k: [3, 5]
        t: [4, 5, 6]
        s: 2
        act_func: 'swish'
        se: False
    mb5:
        c: [112, 120, 128] 
        d: [3, 4, 5, 6, 7, 8]
        k: [3, 5]
        t: [4, 5, 6]
        s: 1
        act_func: 'swish'
        se: True
    mb6:
        c: [192, 200, 208, 216] 
        d: [3, 4, 5, 6, 7, 8]
        k: [3, 5]
        t: [6]
        s: 2
        act_func: 'swish'
        se: True
    mb7:
        c: [216, 224] 
        d: [1, 2]
        k: [3, 5]
        t: [6]
        s: 1
        act_func: 'swish'
        se: True
    last_conv:
        c: [1792, 1984]
        act_func: 'swish'




